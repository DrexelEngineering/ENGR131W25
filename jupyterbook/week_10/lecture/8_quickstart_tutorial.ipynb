{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üìù  Pytorch Quickstart, a deep learning framework for Python ü§ñ\n",
    "\n",
    "## An Engineering Approach to Deep Learning\n",
    "\n",
    "As an engineer we are often expected to use tools without fully understanding them. This is unfortunate, but it is a reality. The alternative is being a mathematician who spends their time exploring minutiae of the theory without doing anything of practical imporance towards a task. \n",
    "\n",
    "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
    "\n",
    "Adapted from [Pytorch Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "\n",
    "```{note}\n",
    "Neural networks generally require a GPU to train. Our server does not have a GPU because they are expensive to run. You can get access to free GPU resource to run this Notebook:\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
    "[NRP JupyterHub](https://jupyterhub-west.nrp-nautilus.io/)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Working with data\n",
    "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
    "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
    "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
    "the ``Dataset``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
    "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
    "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
    "CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
    "use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
    "``target_transform`` to modify the samples and labels respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:01<00:00, 16.5MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 362kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:00<00:00, 6.75MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 11.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),  # Converts images to PyTorch tensors\n",
    ")\n",
    "\n",
    "# Download test data from open datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),  # Converts images to PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
    "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
    "in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read more about [loading data in PyTorch](data_tutorial.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Models\n",
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
    "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
    "operations in the neural network, we move it to the GPU if available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read more about [building neural networks in PyTorch](buildmodel_tutorial.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing the Model Parameters\n",
    "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
    "backpropagates the prediction error to adjust the model's parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also check the model's performance against the test dataset to ensure it is learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
    "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
    "accuracy increase and the loss decrease with every epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299250  [    0/60000]\n",
      "loss: 2.282714  [ 6400/60000]\n",
      "loss: 2.263988  [12800/60000]\n",
      "loss: 2.265477  [19200/60000]\n",
      "loss: 2.231801  [25600/60000]\n",
      "loss: 2.215499  [32000/60000]\n",
      "loss: 2.219949  [38400/60000]\n",
      "loss: 2.184998  [44800/60000]\n",
      "loss: 2.182687  [51200/60000]\n",
      "loss: 2.145585  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 2.138022 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.155730  [    0/60000]\n",
      "loss: 2.140888  [ 6400/60000]\n",
      "loss: 2.077723  [12800/60000]\n",
      "loss: 2.094510  [19200/60000]\n",
      "loss: 2.025862  [25600/60000]\n",
      "loss: 1.975015  [32000/60000]\n",
      "loss: 1.999543  [38400/60000]\n",
      "loss: 1.919724  [44800/60000]\n",
      "loss: 1.932649  [51200/60000]\n",
      "loss: 1.840579  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.842157 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.887067  [    0/60000]\n",
      "loss: 1.855776  [ 6400/60000]\n",
      "loss: 1.726815  [12800/60000]\n",
      "loss: 1.765190  [19200/60000]\n",
      "loss: 1.643301  [25600/60000]\n",
      "loss: 1.604265  [32000/60000]\n",
      "loss: 1.625089  [38400/60000]\n",
      "loss: 1.529855  [44800/60000]\n",
      "loss: 1.559938  [51200/60000]\n",
      "loss: 1.444775  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.465390 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.540911  [    0/60000]\n",
      "loss: 1.513726  [ 6400/60000]\n",
      "loss: 1.353425  [12800/60000]\n",
      "loss: 1.422789  [19200/60000]\n",
      "loss: 1.302089  [25600/60000]\n",
      "loss: 1.304956  [32000/60000]\n",
      "loss: 1.323639  [38400/60000]\n",
      "loss: 1.249234  [44800/60000]\n",
      "loss: 1.284358  [51200/60000]\n",
      "loss: 1.186906  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.208991 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.288033  [    0/60000]\n",
      "loss: 1.282334  [ 6400/60000]\n",
      "loss: 1.106412  [12800/60000]\n",
      "loss: 1.212913  [19200/60000]\n",
      "loss: 1.088997  [25600/60000]\n",
      "loss: 1.115181  [32000/60000]\n",
      "loss: 1.146174  [38400/60000]\n",
      "loss: 1.081228  [44800/60000]\n",
      "loss: 1.118054  [51200/60000]\n",
      "loss: 1.039871  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.054914 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.122897  [    0/60000]\n",
      "loss: 1.140359  [ 6400/60000]\n",
      "loss: 0.949303  [12800/60000]\n",
      "loss: 1.085580  [19200/60000]\n",
      "loss: 0.961126  [25600/60000]\n",
      "loss: 0.989492  [32000/60000]\n",
      "loss: 1.037446  [38400/60000]\n",
      "loss: 0.977653  [44800/60000]\n",
      "loss: 1.011554  [51200/60000]\n",
      "loss: 0.948448  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.956948 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.009725  [    0/60000]\n",
      "loss: 1.049125  [ 6400/60000]\n",
      "loss: 0.844085  [12800/60000]\n",
      "loss: 1.001640  [19200/60000]\n",
      "loss: 0.880502  [25600/60000]\n",
      "loss: 0.902023  [32000/60000]\n",
      "loss: 0.965769  [38400/60000]\n",
      "loss: 0.911617  [44800/60000]\n",
      "loss: 0.938610  [51200/60000]\n",
      "loss: 0.887109  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.890614 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.927336  [    0/60000]\n",
      "loss: 0.985903  [ 6400/60000]\n",
      "loss: 0.769523  [12800/60000]\n",
      "loss: 0.942428  [19200/60000]\n",
      "loss: 0.826356  [25600/60000]\n",
      "loss: 0.838517  [32000/60000]\n",
      "loss: 0.914601  [38400/60000]\n",
      "loss: 0.867979  [44800/60000]\n",
      "loss: 0.886149  [51200/60000]\n",
      "loss: 0.842723  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.842899 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.864195  [    0/60000]\n",
      "loss: 0.938334  [ 6400/60000]\n",
      "loss: 0.713794  [12800/60000]\n",
      "loss: 0.898478  [19200/60000]\n",
      "loss: 0.787520  [25600/60000]\n",
      "loss: 0.790664  [32000/60000]\n",
      "loss: 0.875402  [38400/60000]\n",
      "loss: 0.837619  [44800/60000]\n",
      "loss: 0.846649  [51200/60000]\n",
      "loss: 0.808691  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.806634 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.813529  [    0/60000]\n",
      "loss: 0.900066  [ 6400/60000]\n",
      "loss: 0.670164  [12800/60000]\n",
      "loss: 0.864546  [19200/60000]\n",
      "loss: 0.757976  [25600/60000]\n",
      "loss: 0.753569  [32000/60000]\n",
      "loss: 0.843483  [38400/60000]\n",
      "loss: 0.814915  [44800/60000]\n",
      "loss: 0.815547  [51200/60000]\n",
      "loss: 0.781133  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.777570 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read more about [Training your model](optimization_tutorial.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving Models\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Models\n",
    "\n",
    "The process for loading a model includes re-creating the model structure and loading\n",
    "the state dictionary into it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This model can now be used to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1811\n",
      "Predicted: \"Shirt\", Actual: \"Coat\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAITFJREFUeJzt3X1s1eX9//FXW9rTW1pK6Z0UKKCg3Jkx6RqUL44G6BIjyjbvsoBxEF0xA+Y0XVS8W7ph4oyG4T8TZBG8SQSmWTCIUuIGGFBCyGYFrNyMtkhne2hL7z+/P4jdr3LnddGed2+ej+Qk9Jzz6ufq1U959bTnvBsVBEEgAAAiLNp6AQCAwYkCAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgIkh1gv4rs7OTp06dUopKSmKioqyXg4AwFEQBDp79qxyc3MVHX3pxzl9roBOnTqlvLw862UAAK7SiRMnNHLkyEve3ucKKCUlxXoJ6EVZWVnOmQULFjhnkpKSnDOS9PXXXztnRo0a5ZzxmYB17Ngx58y5c+ecM9L5n0S4ev/9950zvutz5fvTFCaVXZ0r/X/eawW0Zs0aPf/886qurta0adP08ssva8aMGVfMDdQfu/l8XAPx5L/cw/FLCYVCEclIUlxcnHMmPj7eOePzufVZW3t7u3NG8iugvvy1G8m1DcSvW19X2vdeeRLCm2++qZUrV2rVqlX69NNPNW3aNM2bN0+nT5/ujcMBAPqhXimgF154QUuWLNH999+vG264Qa+88ooSExP16quv9sbhAAD9UI8XUGtrq/bv36+ioqL/HSQ6WkVFRdq9e/cF929paVE4HO52AQAMfD1eQGfOnFFHR8cFv2zOyspSdXX1BfcvKytTampq14VnwAHA4GD+QtTS0lLV19d3XU6cOGG9JABABPT4s+AyMjIUExOjmpqabtfX1NQoOzv7gvuHQiHvZywBAPqvHn8EFBcXp+nTp2vHjh1d13V2dmrHjh0qLCzs6cMBAPqpXnkd0MqVK7Vo0SL98Ic/1IwZM/Tiiy+qsbFR999/f28cDgDQD/VKAd111136+uuv9eSTT6q6ulo33nijtm3b5vUqeADAwBQV9LGX7YbDYaWmplovA9/DihUrnDO///3vnTO1tbXOGZ+ROtL5lwW4GjNmjHPGZwRNa2urc8ZneoIkxcbGOmfS0tKcMz7Peq2rq3PO+EzgkPwmQuB/6uvrNXTo0Evebv4sOADA4EQBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEw0j7sKioKOeMz6fz2Wefdc5I0sMPP+ycqaysdM74DOG83ADEy0lISHDOHD582DnjM8DUZ0BoOBx2zkh+w1ITExOdM1VVVc6ZuXPnOmdgg2GkAIA+iQICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgYoj1AnBpPpOtZ8+e7ZwpKipyzkh+k619pk2npKQ4Zzo6Opwzkt8U7cmTJztn0tLSnDNffvmlc2b48OHOGUnq7Ox0zvjseUxMjHPmb3/7m3PmkUcecc5I0hdffOGVw/fDIyAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmogKfiZe9KBwOKzU11XoZfcLMmTOdM6+++qpz5j//+Y9zRpIaGxudM2PGjHHOXHPNNc6Zs2fPOmckqbq62jmTm5vrnMnIyHDOHDx40DnjM8hVkkaMGOGcqaqqcs4kJSU5ZxITE50zZ86ccc5I0i9/+UvnzN69e72ONRDV19dfdsAvj4AAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYYBhpH7ZhwwbnzOTJk50zX331lXNGkrKyspwz7e3tzpm4uLiIZCSpubnZOfPGG284Z37xi184Zzo7O50zsbGxzhlJio52/940Pj4+IhmfQbOXG4h5OZ988olz5uc//7nXsQYihpECAPokCggAYKLHC+ipp55SVFRUt8vEiRN7+jAAgH5uSG+800mTJumDDz7430GG9MphAAD9WK80w5AhQ5Sdnd0b7xoAMED0yu+ADh8+rNzcXI0dO1b33Xefjh8/fsn7trS0KBwOd7sAAAa+Hi+ggoICrV+/Xtu2bdPatWtVWVmpW2655ZJPnSwrK1NqamrXJS8vr6eXBADog3q8gIqLi/Wzn/1MU6dO1bx58/T3v/9ddXV1euutty56/9LSUtXX13ddTpw40dNLAgD0Qb3+7IC0tDRdd911OnLkyEVvD4VCCoVCvb0MAEAf0+uvA2poaNDRo0eVk5PT24cCAPQjPV5AjzzyiMrLy/XVV1/pn//8p+644w7FxMTonnvu6elDAQD6sR7/EdzJkyd1zz33qLa2ViNGjNDNN9+sPXv2aMSIET19KABAP9bjBeQzmBEXN2nSJOdMW1tbRDKS3wuMo6KinDPnzp1zzjQ0NDhnJOmGG25wzpw8edI5s337dufMT3/6U+dMdXW1c0aSNm3a5JxZtGiRc8bnR/M+n9tvvvnGOSOJZ+X2MmbBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMNHrf5AO/pKTk50zPoMaY2JinDOSFASBV85VbGysc8Zn6KmvY8eOOWeGDx/unPHZ73A47JyRpClTpnjlXB0/ftw54zOcNjra73vt7Oxsrxy+Hx4BAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMMA27D4uLi3PO+ExM9p2GHamJ0wkJCc6Z+vp6r2O1trY6ZyZNmuScueGGG5wzLS0tzpkhQ/y+xAsLC50zPnvucw51dHQ4Z4YNG+ac8c3Fx8c7Z5qbm50zAwGPgAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJhgGGmEpKenO2d8hi768B0q6pOLjnb/nqehocE54zO4U/L7mHw+t6FQyDnjM2jWdxhpbW2tc8ZnqK3Px5SUlOScOXfunHNG8tu/MWPGOGc+//xz58xAwCMgAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJhhGGiGjRo1yziQnJztnGhsbnTO+w0g7OzudMz4DVltbW50z8fHxzhnJby989qGtrc05097e7pxpbm52zkhSbGxsRI7l83nyWZvPcFXJb4jp9ddf75xhGCkAABFEAQEATDgX0K5du3TbbbcpNzdXUVFR2rJlS7fbgyDQk08+qZycHCUkJKioqEiHDx/uqfUCAAYI5wJqbGzUtGnTtGbNmovevnr1ar300kt65ZVXtHfvXiUlJWnevHneP4sGAAxMzk9CKC4uVnFx8UVvC4JAL774oh5//HHdfvvtkqQNGzYoKytLW7Zs0d133311qwUADBg9+jugyspKVVdXq6ioqOu61NRUFRQUaPfu3RfNtLS0KBwOd7sAAAa+Hi2g6upqSVJWVla367Oysrpu+66ysjKlpqZ2XfLy8npySQCAPsr8WXClpaWqr6/vupw4ccJ6SQCACOjRAsrOzpYk1dTUdLu+pqam67bvCoVCGjp0aLcLAGDg69ECys/PV3Z2tnbs2NF1XTgc1t69e1VYWNiThwIA9HPOz4JraGjQkSNHut6urKzUgQMHlJ6erlGjRmn58uV67rnndO211yo/P19PPPGEcnNztWDBgp5cNwCgn3MuoH379unWW2/tenvlypWSpEWLFmn9+vV69NFH1djYqKVLl6qurk4333yztm3b5j2bCwAwMDkX0OzZsxUEwSVvj4qK0jPPPKNnnnnmqhY20FxzzTXOGZ/Bnb6DRX34DNT0Gdzp8zH5DDCVpOho959KX+7roSf5rC0xMdHrWD77FwqFnDNxcXHOGZ/9TklJcc5IfsNIMzMzvY41GJk/Cw4AMDhRQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAEw4T8OGH58JuT4TiWNiYpwzCQkJzhnJbyqxz2RrnynLPpO6fbW3tztnfD5PPtOwfY4jScnJyc6ZhoYG54zPxzRkiPt/W74Tqn32jz898/3xCAgAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJhpFGyPDhw50zPsNIfYY7+mppaXHOJCUlOWd8Bov67oPP8MlIDT71+ZjC4bDXsXyGd/oMjW1ubnbO+Az7PHXqlHNGkiZMmOCcGTp0qNexBiMeAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADDBMNIISU1Ndc50dnY6Z86ePeuc8RkqKvkNn4yKinLO+AysHDIkcqd2YmKic8ZnSGik9luSOjo6nDPJycnOmTNnzjhnfIaR1tfXO2ckqampyTmTlpbmdazBiEdAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATDCMNEJ8hpH6aGtrc84EQeB1rNjYWOeMz4DV6Gj375MSEhKcM758Bn7GxMQ4Z1pbW50zPoM7Jb8BtT7HSklJcc74nEM+g1wl6ZtvvnHO+HxdDFY8AgIAmKCAAAAmnAto165duu2225Sbm6uoqCht2bKl2+2LFy9WVFRUt8v8+fN7ar0AgAHCuYAaGxs1bdo0rVmz5pL3mT9/vqqqqroumzZtuqpFAgAGHucnIRQXF6u4uPiy9wmFQsrOzvZeFABg4OuV3wHt3LlTmZmZmjBhgh566CHV1tZe8r4tLS0Kh8PdLgCAga/HC2j+/PnasGGDduzYoT/+8Y8qLy9XcXHxJf/GfFlZmVJTU7sueXl5Pb0kAEAf1OOvA7r77ru7/j1lyhRNnTpV48aN086dOzVnzpwL7l9aWqqVK1d2vR0OhykhABgEev1p2GPHjlVGRoaOHDly0dtDoZCGDh3a7QIAGPh6vYBOnjyp2tpa5eTk9PahAAD9iPOP4BoaGro9mqmsrNSBAweUnp6u9PR0Pf3001q4cKGys7N19OhRPfrooxo/frzmzZvXowsHAPRvzgW0b98+3XrrrV1vf/v7m0WLFmnt2rU6ePCgXnvtNdXV1Sk3N1dz587Vs88+6z2LCQAwMDkX0OzZsy87vPL999+/qgUNVD4F7DN0ccgQ9+eV+AzTlPyGmPoMS/UZcukzwFSSmpqanDM+n6e4uDjnjM/n1mdtkt/n1mfvkpKSnDMNDQ3OmWHDhjlnJKm9vd054/v1NBgxCw4AYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYKLH/yQ3ek5MTEyfPo7P1F+fKdU+6/OdAp2YmOicidTH5DMN2ycj+U3rbm1tdc747IPPhGqfqduS38fks3eDFY+AAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmGAYaYREamBlEATOGZ+1SX4DP32O5ZOJjY11zkhSR0eHc6atrc05E6nzwWdtkt9ATZ/htD777XMc34G78fHxzhnfAbCDEY+AAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmGBqXh8WqWGkPgMXJam5udk54zOo0SfjO2C1paXFK+cqUp8nn8Gdkv8Q00gcx2fvfIarSlJiYqJzxncQ7mDEIyAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmGEYaIT6DO30yw4YNc858/fXXzhnJb+BnpDIdHR3OGclvkKTPsFSfIaE+g1JHjhzpnJGk6upq50xycrJzpqGhwTkTyeG0PnwHwA5GPAICAJiggAAAJpwKqKysTDfddJNSUlKUmZmpBQsWqKKiott9mpubVVJSouHDhys5OVkLFy5UTU1Njy4aAND/ORVQeXm5SkpKtGfPHm3fvl1tbW2aO3euGhsbu+6zYsUKvfvuu3r77bdVXl6uU6dO6c477+zxhQMA+jen3+Zt27at29vr169XZmam9u/fr1mzZqm+vl5/+ctftHHjRv34xz+WJK1bt07XX3+99uzZox/96Ec9t3IAQL92Vb8Dqq+vlySlp6dLkvbv36+2tjYVFRV13WfixIkaNWqUdu/efdH30dLSonA43O0CABj4vAuos7NTy5cv18yZMzV58mRJ55+6GRcXp7S0tG73zcrKuuTTOsvKypSamtp1ycvL810SAKAf8S6gkpISHTp0SG+88cZVLaC0tFT19fVdlxMnTlzV+wMA9A9eL0RdtmyZ3nvvPe3atavbC92ys7PV2tqqurq6bo+CampqlJ2dfdH3FQqFFAqFfJYBAOjHnB4BBUGgZcuWafPmzfrwww+Vn5/f7fbp06crNjZWO3bs6LquoqJCx48fV2FhYc+sGAAwIDg9AiopKdHGjRu1detWpaSkdP1eJzU1VQkJCUpNTdUDDzyglStXKj09XUOHDtXDDz+swsJCngEHAOjGqYDWrl0rSZo9e3a369etW6fFixdLkv70pz8pOjpaCxcuVEtLi+bNm6c///nPPbJYAMDA4VRAQRBc8T7x8fFas2aN1qxZ470onNfa2uqcycrKcs74DiNNSEhwzvj8vi8mJsY5c/bsWeeM5Ld/7e3tzhmfgZo+fF/W8O1LK1y0tbU5Z+Li4pwzkRoYK/mde9/n/0mcxyw4AIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAICJyIzkhdfU3+ho9+8PfCZo+0z8lfzWd+7cOeeMz3ThSE4k9tk/n89TVFSUc6ahocE5I0kjRoxwztTW1jpnhg4d6pxpampyzvhOR/fZc3x/PAICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABggmGkEeIzqNFnyKXPgNDhw4c7ZyTpzJkzzhmf4ZNtbW3OmUgOkYyNjXXOtLS0OGfi4+OdMykpKc4ZyW9obCgUcs50dHQ4Z5KTk50zQ4b4/Vfncx6dPn3a61iDEY+AAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmGAYaYRUVFQ4Z8aPH++cqaysdM40Nzc7ZyS/waI+fIZPdnZ2eh3LZ2ilz5DQ9vZ254zPUNbRo0c7ZyTpiy++cM7k5eU5Z8LhsHMmLi7OORMEgXNGksaMGeOc+eSTT7yONRjxCAgAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJhpFGyGuvveac2bBhg3PGZ+jiokWLnDOS9NxzzzlnGhsbnTPR0e7fJ/kOSvXZP5/1dXR0OGfOnDnjnDl37pxzRpIyMjKcM3V1dc4Zn0Gu//3vf50zhYWFzhn0Ph4BAQBMUEAAABNOBVRWVqabbrpJKSkpyszM1IIFCy74OzezZ89WVFRUt8uDDz7Yo4sGAPR/TgVUXl6ukpIS7dmzR9u3b1dbW5vmzp17wc/1lyxZoqqqqq7L6tWre3TRAID+z+lJCNu2bev29vr165WZman9+/dr1qxZXdcnJiYqOzu7Z1YIABiQrup3QPX19ZKk9PT0bte//vrrysjI0OTJk1VaWqqmpqZLvo+WlhaFw+FuFwDAwOf9NOzOzk4tX75cM2fO1OTJk7uuv/feezV69Gjl5ubq4MGDeuyxx1RRUaF33nnnou+nrKxMTz/9tO8yAAD9lHcBlZSU6NChQ/r444+7Xb906dKuf0+ZMkU5OTmaM2eOjh49qnHjxl3wfkpLS7Vy5cqut8PhsPLy8nyXBQDoJ7wKaNmyZXrvvfe0a9cujRw58rL3LSgokCQdOXLkogUUCoUUCoV8lgEA6MecCigIAj388MPavHmzdu7cqfz8/CtmDhw4IEnKycnxWiAAYGByKqCSkhJt3LhRW7duVUpKiqqrqyVJqampSkhI0NGjR7Vx40b95Cc/0fDhw3Xw4EGtWLFCs2bN0tSpU3vlAwAA9E9OBbR27VpJ519s+v9bt26dFi9erLi4OH3wwQd68cUX1djYqLy8PC1cuFCPP/54jy0YADAwOP8I7nLy8vJUXl5+VQsCAAwOTMPuw3wmM/v461//6pVbv369c8ZnknFbW5tzJi0tzTkjSQkJCc6ZG2+80TlTW1vrnBk/frxzxmf6uHT+ZRaufD5PSUlJzpl9+/Y5Z9A3MYwUAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACYaRRkhUVFREMj4DTH0GT0r6Xn+Q8LtmzJjhnBkxYoRzxmfvJCkrK8s58+WXXzpn2tvbnTNNTU3OmWPHjjlnfLW2tjpnhg4d6px54YUXnDO+oqPdv0f3/XoajHgEBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATfW4WnM8ss/4gUh9XJPfPZ+ZVW1ubc8ZnxpjvLLjm5mbnjM/6fGbB+aytpaXFOePL53Prs75InuMD9f+jSLnS/kUFfWyHT548qby8POtlAACu0okTJzRy5MhL3t7nCqizs1OnTp1SSkrKBd/FhsNh5eXl6cSJE15TdAcK9uE89uE89uE89uG8vrAPQRDo7Nmzys3NvexE8T73I7jo6OjLNqZ0foT7YD7BvsU+nMc+nMc+nMc+nGe9D6mpqVe8D09CAACYoIAAACb6VQGFQiGtWrVKoVDIeimm2Ifz2Ifz2Ifz2Ifz+tM+9LknIQAABod+9QgIADBwUEAAABMUEADABAUEADDRbwpozZo1GjNmjOLj41VQUKBPPvnEekkR99RTTykqKqrbZeLEidbL6nW7du3SbbfdptzcXEVFRWnLli3dbg+CQE8++aRycnKUkJCgoqIiHT582GaxvehK+7B48eILzo/58+fbLLaXlJWV6aabblJKSooyMzO1YMECVVRUdLtPc3OzSkpKNHz4cCUnJ2vhwoWqqakxWnHv+D77MHv27AvOhwcffNBoxRfXLwrozTff1MqVK7Vq1Sp9+umnmjZtmubNm6fTp09bLy3iJk2apKqqqq7Lxx9/bL2kXtfY2Khp06ZpzZo1F7199erVeumll/TKK69o7969SkpK0rx587yGd/ZlV9oHSZo/f36382PTpk0RXGHvKy8vV0lJifbs2aPt27erra1Nc+fOVWNjY9d9VqxYoXfffVdvv/22ysvLderUKd15552Gq+5532cfJGnJkiXdzofVq1cbrfgSgn5gxowZQUlJSdfbHR0dQW5ublBWVma4qshbtWpVMG3aNOtlmJIUbN68uevtzs7OIDs7O3j++ee7rqurqwtCoVCwadMmgxVGxnf3IQiCYNGiRcHtt99ush4rp0+fDiQF5eXlQRCc/9zHxsYGb7/9dtd9/v3vfweSgt27d1sts9d9dx+CIAj+7//+L/j1r39tt6jvoc8/AmptbdX+/ftVVFTUdV10dLSKioq0e/duw5XZOHz4sHJzczV27Fjdd999On78uPWSTFVWVqq6urrb+ZGamqqCgoJBeX7s3LlTmZmZmjBhgh566CHV1tZaL6lX1dfXS5LS09MlSfv371dbW1u382HixIkaNWrUgD4fvrsP33r99deVkZGhyZMnq7S0VE1NTRbLu6Q+N4z0u86cOaOOjg5lZWV1uz4rK0uff/650apsFBQUaP369ZowYYKqqqr09NNP65ZbbtGhQ4eUkpJivTwT1dXVknTR8+Pb2waL+fPn684771R+fr6OHj2q3/3udyouLtbu3bsVExNjvbwe19nZqeXLl2vmzJmaPHmypPPnQ1xcnNLS0rrddyCfDxfbB0m69957NXr0aOXm5urgwYN67LHHVFFRoXfeecdwtd31+QLC/xQXF3f9e+rUqSooKNDo0aP11ltv6YEHHjBcGfqCu+++u+vfU6ZM0dSpUzVu3Djt3LlTc+bMMVxZ7ygpKdGhQ4cGxe9BL+dS+7B06dKuf0+ZMkU5OTmaM2eOjh49qnHjxkV6mRfV538El5GRoZiYmAuexVJTU6Ps7GyjVfUNaWlpuu6663TkyBHrpZj59hzg/LjQ2LFjlZGRMSDPj2XLlum9997TRx991O3Pt2RnZ6u1tVV1dXXd7j9Qz4dL7cPFFBQUSFKfOh/6fAHFxcVp+vTp2rFjR9d1nZ2d2rFjhwoLCw1XZq+hoUFHjx5VTk6O9VLM5OfnKzs7u9v5EQ6HtXfv3kF/fpw8eVK1tbUD6vwIgkDLli3T5s2b9eGHHyo/P7/b7dOnT1dsbGy386GiokLHjx8fUOfDlfbhYg4cOCBJfet8sH4WxPfxxhtvBKFQKFi/fn3wr3/9K1i6dGmQlpYWVFdXWy8ton7zm98EO3fuDCorK4N//OMfQVFRUZCRkRGcPn3aemm96uzZs8Fnn30WfPbZZ4Gk4IUXXgg+++yz4NixY0EQBMEf/vCHIC0tLdi6dWtw8ODB4Pbbbw/y8/ODc+fOGa+8Z11uH86ePRs88sgjwe7du4PKysrggw8+CH7wgx8E1157bdDc3Gy99B7z0EMPBampqcHOnTuDqqqqrktTU1PXfR588MFg1KhRwYcffhjs27cvKCwsDAoLCw1X3fOutA9HjhwJnnnmmWDfvn1BZWVlsHXr1mDs2LHBrFmzjFfeXb8ooCAIgpdffjkYNWpUEBcXF8yYMSPYs2eP9ZIi7q677gpycnKCuLi44Jprrgnuuuuu4MiRI9bL6nUfffRRIOmCy6JFi4IgOP9U7CeeeCLIysoKQqFQMGfOnKCiosJ20b3gcvvQ1NQUzJ07NxgxYkQQGxsbjB49OliyZMmA+ybtYh+/pGDdunVd9zl37lzwq1/9Khg2bFiQmJgY3HHHHUFVVZXdonvBlfbh+PHjwaxZs4L09PQgFAoF48ePD377298G9fX1tgv/Dv4cAwDARJ//HRAAYGCigAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABg4v8BNxuoz9lt4EsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "index = np.random.randint(0, 10000)\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[index][0], test_data[index][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "    plt.imshow(x[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read more about [Saving & Loading your model](saveloadrun_tutorial.html).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
