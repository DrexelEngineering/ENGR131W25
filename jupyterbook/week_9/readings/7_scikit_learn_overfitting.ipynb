{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "346ea8a7",
      "metadata": {},
      "source": [
        "# üéØ Overfitting and Underfitting in Machine Learning: The Balancing Act ‚öñÔ∏è\n",
        "\n",
        "Imagine you‚Äôre an engineer designing a system to recognize faulty components on an assembly line. You train your machine learning model with thousands of images, and it performs flawlessly on the training data. But when you deploy it on the live production line, it starts making mistakes! üò±\n",
        "\n",
        "What went wrong? It‚Äôs the classic battle between Overfitting and Underfitting! Let's explore these two villains that can ruin your machine learning models and learn how to defeat them! ü¶∏‚Äç‚ôÇÔ∏èü¶∏‚Äç‚ôÄÔ∏è\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8486a3f",
      "metadata": {},
      "source": [
        "## üß† What are Overfitting and Underfitting?\n",
        "\n",
        "These are two common problems that occur when training machine learning models:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e36922",
      "metadata": {},
      "source": [
        "### üé© Overfitting: When Your Model is Too Smart...\n",
        "- Definition: The model learns the training data *too well*, even capturing the noise and outliers.\n",
        "- Result: Great performance on training data, but poor generalization to new, unseen data.\n",
        "- In Simple Words: Your model becomes a ‚Äúmemorization machine‚Äù instead of a ‚Äúgeneralization genius.‚Äù\n",
        "- Example:\n",
        "    - Imagine an engineer who memorizes every blueprint, including minor smudges and imperfections. When shown a new, clean blueprint, they get confused because the smudges are missing! üòµ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621aa6d7",
      "metadata": {},
      "source": [
        "### üéà Underfitting: When Your Model is Too Simple...\n",
        "- Definition: The model is too simplistic to capture the patterns in the training data.\n",
        "- Result: Poor performance on both training and testing data.\n",
        "- In Simple Words: Your model is like a student who didn‚Äôt study enough and doesn‚Äôt understand the subject well.\n",
        "- Example:\n",
        "    - A junior engineer who only knows basic formulas and can‚Äôt handle complex problems because they didn‚Äôt learn enough. üòï\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d24ff1",
      "metadata": {},
      "source": [
        "## üîç How to Identify Overfitting and Underfitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3781f6d2",
      "metadata": {},
      "source": [
        "### üìâ Overfitting Symptoms:\n",
        "- High accuracy on training data but low accuracy on testing data.\n",
        "- Large gap between training and validation error.\n",
        "- Example:\n",
        "    - In manufacturing, your model accurately classifies defective parts in historical data but fails on new production batches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db784ec",
      "metadata": {},
      "source": [
        "### üìâ Underfitting Symptoms:\n",
        "- Low accuracy on both training and testing data.\n",
        "- High bias: The model makes overly simplistic assumptions.\n",
        "- Example:\n",
        "    - A model that always predicts the average product quality, regardless of input features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd623f7",
      "metadata": {},
      "source": [
        "## üî® Visualizing Overfitting and Underfitting\n",
        "\n",
        "Imagine fitting a curve to data points:\n",
        "- Underfitting: The model is a straight line that barely touches any points.\n",
        "- Overfitting: The model is a wiggly line that passes through every point, including noise.\n",
        "- Just Right (Generalization): The model captures the underlying pattern without chasing noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b378f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.3, X.shape[0])\n",
        "\n",
        "# Function to plot models\n",
        "\n",
        "\n",
        "def plot_model(degree, title):\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "    plt.scatter(X, y, color=\"blue\", label=\"Data\")\n",
        "    plt.plot(X, y_pred, color=\"red\", label=f\"Degree {degree} Fit\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Underfitting\n",
        "plot_model(1, \"Underfitting (Degree 1)\")\n",
        "\n",
        "# Good Fit\n",
        "plot_model(4, \"Good Fit (Degree 4)\")\n",
        "\n",
        "# Overfitting\n",
        "plot_model(15, \"Overfitting (Degree 15)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9a647b",
      "metadata": {},
      "source": [
        "### üîç What You‚Äôll See:\n",
        "- Underfitting (Degree 1): A straight line missing the patterns.\n",
        "- Good Fit (Degree 4): A smooth curve capturing the pattern.\n",
        "- Overfitting (Degree 15): A complex curve oscillating through every point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e189d00",
      "metadata": {},
      "source": [
        "## üßë‚Äçüîß Engineering Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9850e9e7",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Example 1: Predictive Maintenance\n",
        "- Overfitting: The model memorizes specific failure times instead of learning general patterns from temperature and vibration data.\n",
        "- Underfitting: The model only considers the average lifetime, ignoring valuable sensor data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821d0fb8",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Example 2: Quality Control in Manufacturing\n",
        "- Overfitting: Memorizes defects in historical batches but fails on new designs.\n",
        "- Underfitting: Labels most products as ‚Äúaverage quality,‚Äù missing subtle defects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c9cacc8",
      "metadata": {},
      "source": [
        "## üõ°Ô∏è How to Combat Overfitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f045f9be",
      "metadata": {},
      "source": [
        "### 1. Cross-Validation üß™\n",
        "- Use k-fold cross-validation to ensure the model generalizes well.\n",
        "- Split data into multiple training and testing sets and average the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa0bc741",
      "metadata": {},
      "source": [
        "### 2. Regularization üîó\n",
        "\n",
        "- Add a penalty to the model complexity.\n",
        "- L1 (Lasso) and L2 (Ridge) regularization are commonly used techniques.\n",
        "\n",
        "![](./assets/figures/ridge-lasso.png)\n",
        "\n",
        "- In Scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0bd35b0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = Ridge(alpha=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c3f2d7a",
      "metadata": {},
      "source": [
        "### 3. Early Stopping ‚è∞\n",
        "- Stop training when validation error starts to increase, preventing overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5b1f85",
      "metadata": {},
      "source": [
        "### 4. Pruning üå≥\n",
        "- For decision trees, prune branches that have little importance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86eb2496",
      "metadata": {},
      "source": [
        "### 5. Ensemble Methods üë•\n",
        "- Combine multiple models to reduce overfitting.\n",
        "- Example: Random Forests and Gradient Boosting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3dcc272",
      "metadata": {},
      "source": [
        "### 6. Dropout (for Neural Networks) üíß\n",
        "- Randomly drop neurons during training to prevent memorization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fedcf221",
      "metadata": {},
      "source": [
        "## üõ°Ô∏è How to Combat Underfitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bce66e6",
      "metadata": {},
      "source": [
        "### 1. Increase Model Complexity üîß\n",
        "- Use more complex models (e.g., increase the depth of decision trees or layers in neural networks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e1041c",
      "metadata": {},
      "source": [
        "### 2. Feature Engineering üîç\n",
        "- Add more relevant features or create new ones using domain knowledge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558987c1",
      "metadata": {},
      "source": [
        "### 3. Decrease Regularization ‚ûñ\n",
        "- If regularization is too strong, reduce it to allow the model to learn more patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f175cc",
      "metadata": {},
      "source": [
        "### 4. Ensemble Methods üë•\n",
        "- Using ensemble methods like Random Forests can also improve model complexity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264fa91e",
      "metadata": {},
      "source": [
        "### üîç What You‚Äôll Observe:\n",
        "- The Overfitting Model performs well on training but poorly on testing.\n",
        "- The Ridge Model balances the errors, improving generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60fb17f2",
      "metadata": {},
      "source": [
        "## üéâ Key Takeaways\n",
        "- Overfitting: Model is too complex and memorizes noise.\n",
        "- Underfitting: Model is too simple and misses patterns.\n",
        "- Goal: Achieve Generalization by finding the sweet spot between bias and variance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}