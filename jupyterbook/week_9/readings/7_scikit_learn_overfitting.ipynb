{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346ea8a7",
   "metadata": {},
   "source": [
    "# ğŸ¯ **Overfitting and Underfitting in Machine Learning: The Balancing Act** âš–ï¸\n",
    "\n",
    "Imagine youâ€™re an engineer designing a system to recognize faulty components on an assembly line. You train your machine learning model with thousands of images, and it performs flawlessly on the training data. But when you deploy it on the live production line, it starts making mistakes! ğŸ˜±\n",
    "\n",
    "What went wrong? Itâ€™s the classic battle between **Overfitting** and **Underfitting**! Let's explore these two villains that can ruin your machine learning models and learn how to defeat them! ğŸ¦¸â€â™‚ï¸ğŸ¦¸â€â™€ï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8486a3f",
   "metadata": {},
   "source": [
    "## ğŸ§  **What are Overfitting and Underfitting?**\n",
    "\n",
    "These are two common problems that occur when training machine learning models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e36922",
   "metadata": {},
   "source": [
    "### ğŸ© **Overfitting**: When Your Model is Too Smart...\n",
    "- **Definition:** The model learns the training data *too well*, even capturing the noise and outliers.\n",
    "- **Result:** Great performance on training data, but poor generalization to new, unseen data.\n",
    "- **In Simple Words:** Your model becomes a â€œmemorization machineâ€ instead of a â€œgeneralization genius.â€\n",
    "- **Example:**\n",
    "    - Imagine an engineer who memorizes every blueprint, including minor smudges and imperfections. When shown a new, clean blueprint, they get confused because the smudges are missing! ğŸ˜µ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621aa6d7",
   "metadata": {},
   "source": [
    "### ğŸˆ **Underfitting**: When Your Model is Too Simple...\n",
    "- **Definition:** The model is too simplistic to capture the patterns in the training data.\n",
    "- **Result:** Poor performance on both training and testing data.\n",
    "- **In Simple Words:** Your model is like a student who didnâ€™t study enough and doesnâ€™t understand the subject well.\n",
    "- **Example:**\n",
    "    - A junior engineer who only knows basic formulas and canâ€™t handle complex problems because they didnâ€™t learn enough. ğŸ˜•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d24ff1",
   "metadata": {},
   "source": [
    "## ğŸ” **How to Identify Overfitting and Underfitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781f6d2",
   "metadata": {},
   "source": [
    "### ğŸ“‰ **Overfitting Symptoms:**\n",
    "- **High accuracy on training data** but **low accuracy on testing data**.\n",
    "- **Large gap** between training and validation error.\n",
    "- **Example:**\n",
    "    - In manufacturing, your model accurately classifies defective parts in historical data but fails on new production batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db784ec",
   "metadata": {},
   "source": [
    "### ğŸ“‰ **Underfitting Symptoms:**\n",
    "- **Low accuracy on both training and testing data.**\n",
    "- **High bias:** The model makes overly simplistic assumptions.\n",
    "- **Example:**\n",
    "    - A model that always predicts the average product quality, regardless of input features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd623f7",
   "metadata": {},
   "source": [
    "## ğŸ”¨ **Visualizing Overfitting and Underfitting**\n",
    "\n",
    "Imagine fitting a curve to data points:\n",
    "- **Underfitting:** The model is a straight line that barely touches any points.\n",
    "- **Overfitting:** The model is a wiggly line that passes through every point, including noise.\n",
    "- **Just Right (Generalization):** The model captures the underlying pattern without chasing noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.3, X.shape[0])\n",
    "\n",
    "# Function to plot models\n",
    "\n",
    "\n",
    "def plot_model(degree, title):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = model.predict(X_poly)\n",
    "    plt.scatter(X, y, color=\"blue\", label=\"Data\")\n",
    "    plt.plot(X, y_pred, color=\"red\", label=f\"Degree {degree} Fit\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Underfitting\n",
    "plot_model(1, \"Underfitting (Degree 1)\")\n",
    "\n",
    "# Good Fit\n",
    "plot_model(4, \"Good Fit (Degree 4)\")\n",
    "\n",
    "# Overfitting\n",
    "plot_model(15, \"Overfitting (Degree 15)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a647b",
   "metadata": {},
   "source": [
    "### ğŸ” **What Youâ€™ll See:**\n",
    "- **Underfitting (Degree 1):** A straight line missing the patterns.\n",
    "- **Good Fit (Degree 4):** A smooth curve capturing the pattern.\n",
    "- **Overfitting (Degree 15):** A complex curve oscillating through every point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e189d00",
   "metadata": {},
   "source": [
    "## ğŸ§‘â€ğŸ”§ **Engineering Examples**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850e9e7",
   "metadata": {},
   "source": [
    "### âš™ï¸ **Example 1: Predictive Maintenance**\n",
    "- **Overfitting:** The model memorizes specific failure times instead of learning general patterns from temperature and vibration data.\n",
    "- **Underfitting:** The model only considers the average lifetime, ignoring valuable sensor data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d0fb8",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ **Example 2: Quality Control in Manufacturing**\n",
    "- **Overfitting:** Memorizes defects in historical batches but fails on new designs.\n",
    "- **Underfitting:** Labels most products as â€œaverage quality,â€ missing subtle defects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cacc8",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ **How to Combat Overfitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045f9be",
   "metadata": {},
   "source": [
    "### 1. **Cross-Validation** ğŸ§ª\n",
    "- Use **k-fold cross-validation** to ensure the model generalizes well.\n",
    "- Split data into multiple training and testing sets and average the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bc741",
   "metadata": {},
   "source": [
    "### 2. **Regularization** ğŸ”—\n",
    "\n",
    "- Add a penalty to the model complexity.\n",
    "- **L1 (Lasso)** and **L2 (Ridge)** regularization are commonly used techniques.\n",
    "\n",
    "![](./assets/figures/ridge-lasso.png)\n",
    "\n",
    "- In Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd35b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f2d7a",
   "metadata": {},
   "source": [
    "### 3. **Early Stopping** â°\n",
    "- Stop training when validation error starts to increase, preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b1f85",
   "metadata": {},
   "source": [
    "### 4. **Pruning** ğŸŒ³\n",
    "- For decision trees, prune branches that have little importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb2496",
   "metadata": {},
   "source": [
    "### 5. **Ensemble Methods** ğŸ‘¥\n",
    "- Combine multiple models to reduce overfitting.\n",
    "- Example: **Random Forests** and **Gradient Boosting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcc272",
   "metadata": {},
   "source": [
    "### 6. **Dropout (for Neural Networks)** ğŸ’§\n",
    "- Randomly drop neurons during training to prevent memorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcf221",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ **How to Combat Underfitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce66e6",
   "metadata": {},
   "source": [
    "### 1. **Increase Model Complexity** ğŸ”§\n",
    "- Use more complex models (e.g., increase the depth of decision trees or layers in neural networks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1041c",
   "metadata": {},
   "source": [
    "### 2. **Feature Engineering** ğŸ”\n",
    "- Add more relevant features or create new ones using domain knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558987c1",
   "metadata": {},
   "source": [
    "### 3. **Decrease Regularization** â–\n",
    "- If regularization is too strong, reduce it to allow the model to learn more patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f175cc",
   "metadata": {},
   "source": [
    "### 4. **Ensemble Methods** ğŸ‘¥\n",
    "- Using ensemble methods like Random Forests can also improve model complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fa91e",
   "metadata": {},
   "source": [
    "### ğŸ” **What Youâ€™ll Observe:**\n",
    "- The **Overfitting Model** performs well on training but poorly on testing.\n",
    "- The **Ridge Model** balances the errors, improving generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb17f2",
   "metadata": {},
   "source": [
    "## ğŸ‰ **Key Takeaways**\n",
    "- **Overfitting**: Model is too complex and memorizes noise.\n",
    "- **Underfitting**: Model is too simple and misses patterns.\n",
    "- **Goal**: Achieve **Generalization** by finding the sweet spot between bias and variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
